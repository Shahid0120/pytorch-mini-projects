{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM03xQN3RW5Ugwfp5IaK36C",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shahid0120/pytorch-mini-projects/blob/main/chp_6_deep_dive_into_deep_learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "qk4F7YntLv-n"
      },
      "outputs": [],
      "source": [
        "# Chapter 6\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section 1 - Layer and Modules"
      ],
      "metadata": {
        "id": "rEOMTtF6UXap"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 1"
      ],
      "metadata": {
        "id": "Zg_DLILhUdGK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chp 6.1 Exercise 1\n",
        "# What kinds of problems will occur if you change MySequential to\n",
        "#store modules in a Python list?\n",
        "# Current implementation\n",
        "class MySequential(nn.Module):\n",
        "  def __init__(self, *args):\n",
        "      super().__init__()\n",
        "      for idx, module in enumerate(args):\n",
        "          self.add_module(str(idx), module)\n",
        "\n",
        "  def forward(self, X):\n",
        "    for module in self.children():\n",
        "        X = module(X)\n",
        "    return X\n"
      ],
      "metadata": {
        "id": "DepXzkPiMKOH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sequential with list implementation\n",
        "class MySequential(nn.Module):\n",
        "  def __init__(self, *args):\n",
        "    super().__init__()\n",
        "    self.module_arr = []\n",
        "    for idx, module in enumerate(args):\n",
        "      \"\"\"\n",
        "      Example input:\n",
        "        MySequential(\n",
        "          nn.Linear()\n",
        "          F.relu()\n",
        "          nn.Linear()\n",
        "        )\n",
        "      \"\"\"\n",
        "      self.module_arr.append(module)\n",
        "\n",
        "\n",
        "  def forward(self, X: torch.Tensor) -> torch.Tensor :\n",
        "    for module in enumerate(self.module_arr):\n",
        "        # apply transform sequentially\n",
        "        X = module(X)\n",
        "    return"
      ],
      "metadata": {
        "id": "bwgdQtHpUUkk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "problem : in the constructure we never initialiszed the hiddne layer transformation, thus python is having a problem applky the transformation since it was object was never initialized!"
      ],
      "metadata": {
        "id": "sgoT4O6HUVoE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 2"
      ],
      "metadata": {
        "id": "NAkRxeEjU5N1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q. Implement a module that takes two modules as an argument, say net1 and net2 and returns the concatenated output of both networks in the forward propagation. This is also called a parallel module."
      ],
      "metadata": {
        "id": "dasJsJeWU70u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Random class net1\n",
        "class Net1(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.layer_1 = nn.Linear(in_features=1, out_features=1)\n",
        "    self.relu = nn.ReLU()\n",
        "    self.layer_2 = nn.Linear(in_features=1, out_features=1)\n",
        "\n",
        "  def forward(self, X:torch.Tensor) -> torch.Tensor:\n",
        "    return self.layer_2(self.relu(self.layer_1(X)))\n",
        "\n",
        "# Random class net2\n",
        "class Net2(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.layer_1 = nn.Linear(in_features=1, out_features=1)\n",
        "    self.leaky_relu = nn.LeakyReLU()\n",
        "    self.layer_3 = nn.Linear(in_features=1, out_features=1)\n",
        "\n",
        "  def forward(self, X:torch.Tensor) -> torch.Tensor:\n",
        "    return self.layer_3(self.leaky_relu(self.layer_1(X)))\n",
        "\n",
        "# Parallel Module\n",
        "class ParallelNet1Net2(Net1, Net2):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.net1 = Net1()\n",
        "    self.net2 = Net2()\n",
        "\n",
        "  def concate(self, x:torch.Tensor) -> torch.Tensor:\n",
        "    net1_output = self.net1.forward(x)\n",
        "    net2_ouput = self.net2.forward(x)\n",
        "    return net1_output + net2_ouput\n",
        "\n",
        "x = torch.randn(1)\n",
        "\n",
        "# Net1 initialisation\n",
        "net1 = Net1()\n",
        "net1_x = net1.forward(x).item()\n",
        "\n",
        "# Net2 initialisation\n",
        "net2 = Net2()\n",
        "net2_x = net1.forward(x).item()\n",
        "\n",
        "# Parallel initialisation\n",
        "parallel = ParallelNet1Net2()\n",
        "parallel_x = parallel.concate(x).item()\n",
        "\n",
        "\n",
        "print(f\"X : {x.item()} | Net_1 : {net1_x} | Net_2 : {net2_x} | Parallel : {parallel_x}\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "krzBiBIxVKRY",
        "outputId": "0ecd3528-84ba-4234-fef0-f50dc063b7a9"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X : 0.7477195262908936 | Net_1 : -1.2419242858886719 | Net_2 : -1.2419242858886719 | Parallel : 0.38656148314476013\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Random Recall from Chapter 6"
      ],
      "metadata": {
        "id": "ZuIKr9mCb-iT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Using nn.Sequential\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class RandomNetwork(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.layers = nn.Sequential(\n",
        "        nn.Linear(in_features=1, out_features=10),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(in_features=10, out_features=10),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(in_features=10, out_features=1)\n",
        "    )\n",
        "\n",
        "  def forward(self, X:torch.Tensor) -> torch.Tensor:\n",
        "    return self.layers(X)\n",
        "\n",
        "x = torch.rand(1)\n",
        "random_network = RandomNetwork()\n",
        "random_network_x = random_network.forward(x)\n",
        "\n",
        "print(f\"X : {x.item()} | Random Network : {random_network_x.item()}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iXZOeF8_cDLC",
        "outputId": "68bd8d33-2a62-4d98-ca9a-a7034812b459"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X : 0.6129076480865479 | Random Network : -0.4373367428779602\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section 2 - Paramter Management"
      ],
      "metadata": {
        "id": "m0nq6d8Id8pi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RandomNetwork(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.layers = nn.Sequential(\n",
        "        nn.Linear(in_features=1, out_features=10),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(in_features=10, out_features=10),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(in_features=10, out_features=1)\n",
        "    )\n",
        "\n",
        "  def forward(self, X:torch.Tensor) -> torch.Tensor:\n",
        "    return self.layers(X)\n",
        "\n",
        "X = torch.rand(1)\n",
        "random_network = RandomNetwork()\n",
        "random_network_x = random_network.forward(X)\n",
        "\n",
        "print(f\"X : {x.item()} | Random Network : {random_network_x.item()}\")\n",
        "print(f\"Paramter of Random Network : {random_network.state_dict()}\")\n",
        "print(f\"Bias Paramter for layer 1 {random_network.layers[0].bias.data}\")\n",
        "\n",
        "for index,layer in enumerate(random_network.layers):\n",
        "  try:\n",
        "    print(f\"Index : {index} | Layer : {layer} \\n {random_network.layers[index].bias.data}\")\n",
        "  except:\n",
        "    continue\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PQ7CBUJ2d__w",
        "outputId": "0911d591-d069-420e-9901-8e744a4b283b"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X : 0.6129076480865479 | Random Network : 0.3765067756175995\n",
            "Paramter of Random Network : OrderedDict([('layers.0.weight', tensor([[ 0.0495],\n",
            "        [-0.2563],\n",
            "        [ 0.2484],\n",
            "        [ 0.3130],\n",
            "        [-0.0187],\n",
            "        [-0.9413],\n",
            "        [-0.1802],\n",
            "        [ 0.9323],\n",
            "        [ 0.5465],\n",
            "        [-0.3764]])), ('layers.0.bias', tensor([ 0.7994, -0.2375, -0.9554, -0.1615,  0.6156, -0.6021,  0.5208,  0.0344,\n",
            "        -0.2199, -0.0063])), ('layers.2.weight', tensor([[-0.0623,  0.0986, -0.0783,  0.1477,  0.1602, -0.0996,  0.1328,  0.1704,\n",
            "          0.0602,  0.1955],\n",
            "        [ 0.2854,  0.1868,  0.3144,  0.0576,  0.1373,  0.0299,  0.1351, -0.0996,\n",
            "         -0.2343,  0.0757],\n",
            "        [-0.2132,  0.1305,  0.1689,  0.2491, -0.2253,  0.0010,  0.0974,  0.0106,\n",
            "          0.2375,  0.1194],\n",
            "        [ 0.1727,  0.2207,  0.1660, -0.0159,  0.1599, -0.2343, -0.1310,  0.1077,\n",
            "         -0.2478,  0.2949],\n",
            "        [ 0.1477, -0.1895, -0.0634, -0.0521,  0.0963, -0.0011, -0.1975,  0.0981,\n",
            "         -0.2022, -0.2394],\n",
            "        [ 0.2704,  0.2799,  0.2693, -0.1239,  0.2413, -0.1975, -0.1877, -0.2943,\n",
            "          0.1302,  0.2083],\n",
            "        [-0.2043,  0.2813, -0.2687, -0.0588, -0.2846,  0.2686,  0.0235, -0.0431,\n",
            "          0.0026, -0.1860],\n",
            "        [-0.0656, -0.2292, -0.0238, -0.1945,  0.1198, -0.1039, -0.2218, -0.0201,\n",
            "         -0.2353, -0.2890],\n",
            "        [-0.0551, -0.3109, -0.0136,  0.2768,  0.2610,  0.0456, -0.1394, -0.1696,\n",
            "          0.3151, -0.2890],\n",
            "        [ 0.2341,  0.0722,  0.1823, -0.2390, -0.0100,  0.0081,  0.0388,  0.1913,\n",
            "          0.0034,  0.1656]])), ('layers.2.bias', tensor([ 0.2703, -0.0895,  0.0602,  0.0928,  0.0198,  0.2409, -0.0249, -0.0996,\n",
            "         0.0957, -0.1814])), ('layers.4.weight', tensor([[-0.1772, -0.1195,  0.2007,  0.1350,  0.2099,  0.1835,  0.1417, -0.2789,\n",
            "          0.1103,  0.1573]])), ('layers.4.bias', tensor([0.3142]))])\n",
            "Bias Paramter for layer 1 tensor([ 0.7994, -0.2375, -0.9554, -0.1615,  0.6156, -0.6021,  0.5208,  0.0344,\n",
            "        -0.2199, -0.0063])\n",
            "Index : 0 | Layer : Linear(in_features=1, out_features=10, bias=True) \n",
            " tensor([ 0.7994, -0.2375, -0.9554, -0.1615,  0.6156, -0.6021,  0.5208,  0.0344,\n",
            "        -0.2199, -0.0063])\n",
            "Index : 2 | Layer : Linear(in_features=10, out_features=10, bias=True) \n",
            " tensor([ 0.2703, -0.0895,  0.0602,  0.0928,  0.0198,  0.2409, -0.0249, -0.0996,\n",
            "         0.0957, -0.1814])\n",
            "Index : 4 | Layer : Linear(in_features=10, out_features=1, bias=True) \n",
            " tensor([0.3142])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section 5 - Custom Layers"
      ],
      "metadata": {
        "id": "e0YivEewl7cW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Custom Layers - No paramters\n",
        "class MeanLayer(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "  def forward(self, X:torch.Tensor) -> torch.Tensor :\n",
        "    return X - X.mean()\n",
        "\n",
        "net = nn.Sequential(\n",
        "    nn.LazyLinear(out_features=25),\n",
        "    MeanLayer(),\n",
        "    nn.LazyLinear(out_features=1)\n",
        ")\n",
        "\n",
        "X = torch.rand(1, 9)\n",
        "\n",
        "print(f\"X : {X} | Net : {net(X).item()}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cIXcsYpsmBer",
        "outputId": "2bc2a2e7-ec94-4578-cc80-3e11ed42b918"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X : tensor([[0.6308, 0.6679, 0.0726, 0.4424, 0.5295, 0.7657, 0.4185, 0.4542, 0.8662]]) | Net : -0.2034977674484253\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Custom Layer - With paramters\n",
        "from torch.nn.parameter import Parameter\n",
        "\n",
        "class LinearFc(nn.Module):\n",
        "  def __init__(self, in_units, units):\n",
        "    super().__init__()\n",
        "    self.weight = nn.Parameter(data=torch.rand(in_units, units))\n",
        "    self.bias = nn.Parameter(data=torch.rand(units,))\n",
        "\n",
        "  def forward(self, X:torch.Tensor) -> torch.Tensor :\n",
        "    return torch.matmul(self.weight, X) + self.bias\n",
        "\n",
        "network = nn.Sequential(\n",
        "    nn.LazyLinear(10),\n",
        "    LinearFc(10, 1),\n",
        "    nn.ReLU()\n",
        ")\n",
        "\n",
        "X = torch.rand(1, 10)\n",
        "\n",
        "print(f\"X:\\n  \\n{X}\\n  \\nNetwork:\\n \\n{network(X)}\\n\")\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lE2N0QNfnPwx",
        "outputId": "0ce65c7c-de4b-42b5-e95f-358758391dba"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X:\n",
            "  \n",
            "tensor([[0.6675, 0.4793, 0.1532, 0.6025, 0.8826, 0.1669, 0.3418, 0.4608, 0.7979,\n",
            "         0.3946]])\n",
            "  \n",
            "Network:\n",
            " \n",
            "tensor([[0.6735, 0.6340, 0.5587, 0.6629, 0.5646, 0.6866, 0.6228, 0.5700, 0.6023,\n",
            "         0.5785],\n",
            "        [0.6843, 0.6336, 0.5370, 0.6707, 0.5446, 0.7012, 0.6193, 0.5516, 0.5930,\n",
            "         0.5625],\n",
            "        [0.8281, 0.6288, 0.2490, 0.7745, 0.2787, 0.8943, 0.5723, 0.3062, 0.4689,\n",
            "         0.3491],\n",
            "        [0.7313, 0.6321, 0.4430, 0.7046, 0.4577, 0.7642, 0.6039, 0.4715, 0.5524,\n",
            "         0.4928],\n",
            "        [0.6393, 0.6351, 0.6271, 0.6382, 0.6278, 0.6407, 0.6340, 0.6283, 0.6318,\n",
            "         0.6292],\n",
            "        [0.7076, 0.6329, 0.4905, 0.6874, 0.5016, 0.7323, 0.6117, 0.5120, 0.5729,\n",
            "         0.5280],\n",
            "        [0.7584, 0.6312, 0.3886, 0.7242, 0.4076, 0.8006, 0.5951, 0.4251, 0.5290,\n",
            "         0.4526],\n",
            "        [0.6743, 0.6340, 0.5571, 0.6635, 0.5631, 0.6877, 0.6225, 0.5687, 0.6016,\n",
            "         0.5774],\n",
            "        [0.8321, 0.6287, 0.2410, 0.7773, 0.2713, 0.8996, 0.5710, 0.2994, 0.4654,\n",
            "         0.3432],\n",
            "        [0.7029, 0.6330, 0.4998, 0.6841, 0.5102, 0.7261, 0.6132, 0.5199, 0.5769,\n",
            "         0.5349]], grad_fn=<ReluBackward0>)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MjWIT3ed1K9f"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}